{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e962d2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniforge/envs/omic_env/lib/python3.10/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3  # 指定脚本由 python3 解释运行\n",
    "import os  # 导入 os 模块以便进行文件和路径操作\n",
    "import sys  # 导入 sys 模块以便处理系统相关操作\n",
    "import math  # 导入 math 模块以便使用数学函数\n",
    "import numpy as np  # 导入 numpy 并别名为 np 用于数值计算\n",
    "import anndata  # 导入 anndata 用于读取和处理 .h5ad 文件\n",
    "import scanpy as sc  # 导入 scanpy 用于 scRNA-seq 常用预处理\n",
    "import scipy.sparse as sp  # 导入 scipy.sparse 以识别稀疏矩阵类型\n",
    "import torch  # 导入 PyTorch 以便构建与训练模型\n",
    "import torch.nn as nn  # 导入神经网络模块并命名为 nn\n",
    "import torch.nn.functional as F  # 导入函数式接口并命名为 F\n",
    "from torch.utils.data import Dataset, DataLoader  # 从 PyTorch 导入 Dataset 与 DataLoader\n",
    "from sklearn.mixture import GaussianMixture  # 导入高斯混合模型用于聚类\n",
    "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score  # 导入 ARI 与 NMI 评估函数\n",
    "import matplotlib.pyplot as plt  # 导入 matplotlib 用于绘图\n",
    "from matplotlib import cm  # 导入 matplotlib 的 colormap 工具"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c926c38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 模型训练使用设备：cuda:3\n"
     ]
    }
   ],
   "source": [
    "# device 选择：优先 CUDA:3，其次 MPS，否则 CPU\n",
    "device = torch.device(  # 定义要使用的设备\n",
    "    \"cuda:3\" if torch.cuda.is_available() else  # 如果 CUDA 可用则选择 cuda:3\n",
    "    \"mps\" if torch.backends.mps.is_available() else  # 否则如果 MPS 可用则选择 mps\n",
    "    \"cpu\"  # 否则退回到 CPU\n",
    ")  # 结束 device 定义\n",
    "print(f\"✅ 模型训练使用设备：{device}\")  # 打印将要使用的设备以便确认"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81496162",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnnData object with n_obs × n_vars = 5780 × 27876\n",
       "    obs: 'clusters', 'palantir_pseudotime', 'palantir_diff_potential'\n",
       "    var: 'palantir'\n",
       "    uns: 'clusters_colors', 'palantir_branch_probs_cell_types'\n",
       "    obsm: 'MAGIC_imputed_data', 'X_tsne', 'palantir_branch_probs'\n",
       "    layers: 'spliced', 'unspliced'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = '/share/2025_undergraduate/20251106/setty_bone_marrow.h5ad'  # 指定输入 h5ad 文件路径\n",
    "adata = anndata.read_h5ad(file_path)                          # 读取 h5ad 文件为 AnnData 对象\n",
    "adata.var_names_make_unique()  # 确保基因名唯一以避免下标混淆\n",
    "adata.obs_names_make_unique()  # 确保细胞名唯一以避免下标混淆\n",
    "adata  # 显示 adata（Jupyter 中会展示对象摘要）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "911f449e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QC 基因注释：线粒体基因（人用 \"MT-\"，鼠用 \"Mt-\"）\n",
    "adata.var[\"mt\"] = adata.var_names.str.startswith(\"MT-\")  # 标记以 MT- 开头的基因为线粒体基因\n",
    "# 核糖体基因注释\n",
    "adata.var[\"ribo\"] = adata.var_names.str.startswith((\"RPS\", \"RPL\"))  # 标记以 RPS 或 RPL 开头的基因为核糖体基因\n",
    "# 血红蛋白基因注释\n",
    "adata.var[\"hb\"] = adata.var_names.str.contains(\"^HB[^(P)]\")  # 标记血红蛋白类基因（粗略匹配）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30012664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算 QC 指标（会将结果写入 adata.obs），并对计数做 log1p（这里仅用于 QC 统计）\n",
    "sc.pp.calculate_qc_metrics(adata, qc_vars=[\"mt\", \"ribo\", \"hb\"], inplace=True, log1p=True)  # 计算 QC 指标并写入 adata.obs\n",
    "# 过滤低质量细胞：至少 100 个基因\n",
    "sc.pp.filter_cells(adata, min_genes=100)  # 过滤掉基因数少于 min_genes 的细胞\n",
    "# 过滤低表达基因：至少在 3 个细胞中表达\n",
    "sc.pp.filter_genes(adata, min_cells=3)  # 过滤掉在少于 min_cells 个细胞中表达的基因\n",
    "# 使用 Scrublet 预测并过滤双细胞（注意：scanpy.scrublet 需要安装 scrublet 包并支持）\n",
    "try:\n",
    "    sc.pp.scrublet(adata)   # 尝试运行 Scrublet 双细胞检测（若 scanpy 版本支持）\n",
    "except Exception:  # 如果 scrublet 不可用或调用失败\n",
    "    pass  # 忽略 Scrublet 步骤（继续运行）\n",
    "\n",
    "# 归一化到中位数总 counts（临时用于 HVG 检测）\n",
    "sc.pp.normalize_total(adata)  # 将每个细胞的总 counts 缩放到同一水平（默认 target_sum=1e4）\n",
    "# 对数化数据以便下游统计稳定\n",
    "sc.pp.log1p(adata)  # 对数据执行 log1p 变换\n",
    "# 计算高变基因（HVG），默认会在 adata.var['highly_variable'] 中标记\n",
    "sc.pp.highly_variable_genes(adata)  # 识别高度可变基因\n",
    "# 仅保留 HVG 子集以便后续分析（注意：训练 ZINB-VAE 时我们会使用原始计数矩阵的这些基因）\n",
    "adata = adata[:,adata.var['highly_variable']==True]  # 根据 HVG 掩码对 adata 进行基因子集化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fedae047",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int):  # 设置随机种子以提高可重复性\n",
    "    np.random.seed(seed)  # 设置 numpy 随机种子\n",
    "    torch.manual_seed(seed)  # 设置 PyTorch CPU 随机种子\n",
    "    try:\n",
    "        torch.cuda.manual_seed_all(seed)  # 如果有 CUDA 则设置所有 GPU 随机种子\n",
    "    except Exception:\n",
    "        pass  # 若设置失败则忽略"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6bf6bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_to_dense_array(X):  # 将 scipy 稀疏矩阵转为 numpy 密集数组\n",
    "    if sp.issparse(X):  # 如果输入为稀疏矩阵\n",
    "        X = X.toarray()  # 转为密集数组\n",
    "    return np.asarray(X)  # 返回 numpy ndarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55f508a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CountsDataset(Dataset):  # 自定义 PyTorch Dataset，用于按批次提供原始计数与 size_factors\n",
    "    def __init__(self, counts: np.ndarray, size_factors: np.ndarray):  # 构造函数，接收计数矩阵与对应的 size_factors\n",
    "        assert counts.shape[0] == size_factors.shape[0]  # 确保细胞数一致\n",
    "        self.counts = counts.astype(np.float32)  # 存储计数并转换为 float32\n",
    "        self.size_factors = size_factors.astype(np.float32)  # 存储 size_factors 并转换为 float32\n",
    "    def __len__(self):  # 返回样本数量（细胞数）\n",
    "        return self.counts.shape[0]  # 返回计数矩阵的行数\n",
    "    def __getitem__(self, idx):  # 返回第 idx 个样本（计数向量, size_factor）\n",
    "        x = self.counts[idx]  # 获取第 idx 个细胞的计数向量\n",
    "        sf = self.size_factors[idx]  # 获取对应的 size_factor\n",
    "        return x, sf  # 返回计数和 size_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "27cd39db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zinb_negative_log_likelihood(x, mu, theta, pi_logits, eps=1e-8):  # 完整且数值稳定的 ZINB 负对数似然实现\n",
    "    if theta.ndim == 1:  # 如果 theta 是 1D（每基因一个参数）\n",
    "        theta = theta.unsqueeze(0).expand_as(mu)  # 扩展为与 mu 相同形状以便逐元素计算\n",
    "    theta = torch.clamp(theta, min=eps)  # 限制 theta 最小值避免数值问题\n",
    "    mu = torch.clamp(mu, min=eps)  # 限制 mu 最小值避免 log(0)\n",
    "    # NB log pmf 计算（准确稳定的形式）\n",
    "    lg1 = torch.lgamma(theta + x) - torch.lgamma(theta) - torch.lgamma(x + 1.0)  # Gamma 函数组合项\n",
    "    lg2 = theta * (torch.log(theta + eps) - torch.log(mu + theta + eps)) + x * (torch.log(mu + eps) - torch.log(mu + theta + eps))  # 与 mu, theta 相关的对数项\n",
    "    log_nb = lg1 + lg2  # 负二项分布的对数概率\n",
    "    pi = torch.sigmoid(pi_logits)  # 将 logits 转换为零膨胀概率 pi\n",
    "    # 处理零事件：log(pi + (1-pi) * exp(log_nb_zero)) 的稳定计算\n",
    "    log_pi = torch.log(torch.clamp(pi, min=eps))  # log(pi)\n",
    "    log_1_minus_pi = torch.log(torch.clamp(1 - pi, min=eps))  # log(1-pi)\n",
    "    log_nb_zero = log_nb  # 当 x == 0 时 log_nb 即为 NB 在 0 的对数概率\n",
    "    a = log_pi  # a = log(pi)\n",
    "    b = log_1_minus_pi + log_nb_zero  # b = log(1-pi) + log_nb_zero\n",
    "    max_ab = torch.maximum(a, b)  # 取最大值用于数值稳定的 log-sum-exp\n",
    "    log_zinb_zero = max_ab + torch.log(torch.exp(a - max_ab) + torch.exp(b - max_ab) + eps)  # 稳定计算 log(pi + (1-pi)*exp(log_nb_zero))\n",
    "    # 非零计数时的 log 概率\n",
    "    log_zinb_nonzero = log_1_minus_pi + log_nb  # log((1-pi) * p_nb(x))\n",
    "    # 选择零或非零的 log 概率\n",
    "    log_prob = torch.where(x < 0.5, log_zinb_zero, log_zinb_nonzero)  # 对于 x < 0.5 视为 0\n",
    "    nll = -log_prob  # 负对数似然\n",
    "    return nll.sum(dim=1).mean()  # 对基因求和再对批次求平均并返回"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b64c9bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ZINBVAE(nn.Module):  # 完整的 ZINB-VAE 模型实现（编码器->潜变量->解码器输出 ZINB 参数）\n",
    "    def __init__(self, n_genes: int, n_latent: int = 10, hidden_dims=(256,128), theta_init=1.0, dropout=0.2):  # 构造函数\n",
    "        super().__init__()  # 调用父类构造函数\n",
    "        self.n_genes = n_genes  # 保存基因数\n",
    "        self.n_latent = n_latent  # 保存潜变量维度\n",
    "        # 构建编码器 MLP\n",
    "        enc_layers = []  # 存放编码器层的列表————\n",
    "        input_dim = n_genes  # 编码器输入维度为基因数\n",
    "        for h in hidden_dims:  # 以 hidden_dims 构建多层编码器\n",
    "            enc_layers.append(nn.Linear(input_dim, h))  # 添加线性层\n",
    "            enc_layers.append(nn.ReLU())  # 添加 ReLU 激活\n",
    "            enc_layers.append(nn.Dropout(dropout))  # 添加 Dropout 做正则化\n",
    "            input_dim = h  # 更新下一层输入维度\n",
    "        self.encoder = nn.Sequential(*enc_layers)  # 将编码器层封装为 Sequential\n",
    "        self.enc_mu = nn.Linear(input_dim, n_latent)  # 编码器输出 z 的均值\n",
    "        self.enc_logvar = nn.Linear(input_dim, n_latent)  # 编码器输出 z 的对数方差\n",
    "        # 构建解码器 MLP（对称结构）\n",
    "        dec_layers = []  # 存放解码器隐藏层\n",
    "        input_dim = n_latent  # 解码器输入维度为潜变量维度\n",
    "        for h in reversed(hidden_dims):  # 逆序构建隐藏层以对称编码器\n",
    "            dec_layers.append(nn.Linear(input_dim, h))  # 添加线性层\n",
    "            dec_layers.append(nn.ReLU())  # 添加 ReLU 激活\n",
    "            dec_layers.append(nn.Dropout(dropout))  # 添加 Dropout\n",
    "            input_dim = h  # 更新下一层输入维度\n",
    "        self.decoder_hidden = nn.Sequential(*dec_layers)  # 封装解码器隐藏层\n",
    "        self.dec_mu = nn.Linear(input_dim, n_genes)  # 解码器输出 log_mu 的线性头（未结合 size_factors）\n",
    "        self.dec_pi = nn.Linear(input_dim, n_genes)  # 解码器输出 pi logits 的线性头\n",
    "        # 可学习的每基因 dispersion 参数 theta（初始化为 theta_init）\n",
    "        self.theta = nn.Parameter(torch.ones(n_genes) * theta_init)  # 参数 theta，可学习\n",
    "        \n",
    "    def encode(self, x):  # 编码函数，x 已按 log1p(counts/size_factors) 变换\n",
    "        h = self.encoder(x)  # 通过编码器获得隐层表示 h\n",
    "        return self.enc_mu(h), self.enc_logvar(h)  # 返回 z 的均值与对数方差\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):  # 重参数化采样函数\n",
    "        std = torch.exp(0.5 * logvar)  # 计算标准差\n",
    "        eps = torch.randn_like(std)  # 生成正态噪声\n",
    "        return mu + eps * std  # 返回采样后的 z\n",
    "    \n",
    "    def decode(self, z, size_factors):  # 解码函数，返回 pi_logits、mu、theta\n",
    "        h = self.decoder_hidden(z)  # 通过解码器隐藏层得到 h\n",
    "        pi_logits = self.dec_pi(h)  # 计算 pi 的 logits（batch, genes）\n",
    "        log_mu = self.dec_mu(h)  # 计算基因的 log_mu（batch, genes）\n",
    "        sf_log = torch.log(torch.clamp(size_factors, min=1e-8)).unsqueeze(1)  # 计算 size_factors 的对数并扩展为 (batch,1)\n",
    "        mu = torch.exp(log_mu + sf_log)  # 将 log_mu 与 size_factors 对数相加后 exponentiate 得到最终的 mu\n",
    "        theta = F.softplus(self.theta) + 1e-4  # 用 softplus 确保 theta 为正并加上微小常数避免数值问题\n",
    "        return pi_logits, mu, theta  # 返回解码得到的 ZINB 参数\n",
    "    \n",
    "    def forward(self, x, size_factors):  # 前向函数：计算 NLL、KL 并返回 z_mu\n",
    "        sf = size_factors.unsqueeze(1)  # 扩展 size_factors 到 (batch,1)\n",
    "        x_enc = torch.log1p(x / (sf + 1e-8))  # 使用 log1p(counts/size_factors) 作为编码器的输入变换\n",
    "        z_mu, z_logvar = self.encode(x_enc)  # 得到 z 的均值与对数方差\n",
    "        z = self.reparameterize(z_mu, z_logvar)  # 重参数化采样 z\n",
    "        pi_logits, mu, theta = self.decode(z, size_factors)  # 解码得到 ZINB 参数\n",
    "        nll = zinb_negative_log_likelihood(x, mu, theta, pi_logits)  # 计算 ZINB 的负对数似然\n",
    "        kl = -0.5 * torch.sum(1 + z_logvar - z_mu.pow(2) - z_logvar.exp(), dim=1).mean()  # 计算 q(z|x) 与标准正态的 KL 并对批次均值化\n",
    "        return nll, kl, z_mu  # 返回 NLL、KL、z 的均值表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bb949298",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model: ZINBVAE, dataloader: DataLoader, optimizer, device, n_epochs=200, kl_weight=1.0, verbose=True):  # 完整训练循环\n",
    "    model.to(device)  # 将模型移动到指定设备\n",
    "    for epoch in range(1, n_epochs + 1):  # 迭代每个 epoch\n",
    "        model.train()  # 切换到训练模式（启用 dropout）\n",
    "        epoch_nll = 0.0  # 累计 NLL\n",
    "        epoch_kl = 0.0  # 累计 KL\n",
    "        batch_count = 0  # 批次数计数器\n",
    "        for x_batch, sf_batch in dataloader:  # 遍历数据加载器\n",
    "            x_batch = torch.tensor(x_batch, device=device) if not isinstance(x_batch, torch.Tensor) else x_batch.to(device)  # 将批次计数送入设备\n",
    "            sf_batch = torch.tensor(sf_batch, device=device) if not isinstance(sf_batch, torch.Tensor) else sf_batch.to(device)  # 将 size_factors 送入设备\n",
    "            optimizer.zero_grad()  # 清零梯度\n",
    "            nll, kl, _ = model(x_batch, sf_batch)  # 前向计算得到 NLL 与 KL\n",
    "            loss = nll + kl_weight * kl  # 总损失 = NLL + kl_weight * KL\n",
    "            loss.backward()  # 反向传播计算梯度\n",
    "            optimizer.step()  # 更新参数\n",
    "            epoch_nll += nll.item()  # 累加 NLL\n",
    "            epoch_kl += kl.item()  # 累加 KL\n",
    "            batch_count += 1  # 增加批次计数\n",
    "        if verbose and (epoch % 10 == 0 or epoch == 1):  # 每 10 个 epoch 或第 1 个 epoch 打印一次信息\n",
    "            print(f\"Epoch {epoch}/{n_epochs}, avg_nll={epoch_nll/batch_count:.4f}, avg_kl={epoch_kl/batch_count:.4f}\")  # 打印平均 NLL 与 KL\n",
    "    return model  # 返回训练后的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4b358577",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latent_embeddings(model: ZINBVAE, counts: np.ndarray, size_factors: np.ndarray, device, batch_size=256) -> np.ndarray:  # 计算所有细胞的 z_mu 嵌入\n",
    "    model.to(device)  # 将模型移动到设备\n",
    "    model.eval()  # 切换到评估模式禁用 dropout\n",
    "    dataset = CountsDataset(counts, size_factors)  # 用 CountsDataset 包装数据\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)  # 创建推断用 DataLoader（不打乱）\n",
    "    zs = []  # 用于收集 z_mu 的列表\n",
    "    with torch.no_grad():  # 关闭梯度计算以节省显存\n",
    "        for x_batch, sf_batch in dataloader:  # 遍历每个批次\n",
    "            x_batch = torch.tensor(x_batch, device=device) if not isinstance(x_batch, torch.Tensor) else x_batch.to(device)  # 转为 tensor 并移动到设备\n",
    "            sf_batch = torch.tensor(sf_batch, device=device) if not isinstance(sf_batch, torch.Tensor) else sf_batch.to(device)  # 转为 tensor 并移动到设备\n",
    "            x_enc = torch.log1p(x_batch / (sf_batch.unsqueeze(1) + 1e-8))  # 计算 encoder 输入变换\n",
    "            z_mu, _ = model.encode(x_enc)  # 通过编码器获取 z 的均值\n",
    "            zs.append(z_mu.cpu().numpy())  # 将结果移回 CPU 并转为 numpy 后追加\n",
    "    zs = np.vstack(zs)  # 将所有批次的嵌入垂直堆叠为数组\n",
    "    return zs  # 返回嵌入矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9ec58c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_full_from_prelude(adata_prelude, raw_h5ad_path, device, out_h5ad=None, latent_dim=10, batch_size=128, n_epochs=200, lr=1e-3, seed=42, n_top_genes=2000):  # 主流程函数签名\n",
    "    set_seed(seed)  # 设置随机种子以保证可重复性\n",
    "    # 1) 确保 prelude 中的 adata 是经过 HVG 筛选的子集（前置代码已完成该步）\n",
    "    adata_hvg = adata_prelude  # 将前置 adata 视为 HVG 子集\n",
    "    # 2) 从原始 h5ad 中提取这些 HVG 对应的原始计数（未归一化 / 未 log1p）\n",
    "    raw_adata = anndata.read_h5ad(raw_h5ad_path)  # 读取原始 h5ad 以获取原始计数\n",
    "    raw_adata.var_names_make_unique()  # 确保基因名唯一\n",
    "    raw_adata.obs_names_make_unique()  # 确保细胞名唯一\n",
    "    hvgs = list(adata_hvg.var_names)  # 获取 HVG 列表\n",
    "    if set(hvgs).issubset(set(raw_adata.var_names)):  # 若 HVG 为原始数据子集\n",
    "        hvgs_idx = [list(raw_adata.var_names).index(g) for g in hvgs]  # 在原始 var 中找到索引\n",
    "        raw_counts = raw_adata.X[:, hvgs_idx]  # 提取 HVG 的原始计数矩阵\n",
    "    else:\n",
    "        common_genes = [g for g in hvgs if g in raw_adata.var_names]  # 计算交集基因\n",
    "        if len(common_genes) == 0:  # 若无交集则报错\n",
    "            raise ValueError(\"前置 adata 的基因与原始 h5ad 无重叠，请检查基因命名一致性\")  # 提示用户检查基因名\n",
    "        hvgs_idx = [list(raw_adata.var_names).index(g) for g in common_genes]  # 找到交集基因索引\n",
    "        raw_counts = raw_adata.X[:, hvgs_idx]  # 提取对应计数\n",
    "        adata_hvg = adata_hvg[:, common_genes]  # 更新 prelude adata 以对齐基因\n",
    "\n",
    "    # 3) 确保 raw_counts 为密集矩阵并为 float32\n",
    "    if sp.issparse(raw_counts):  # 如果原始计数是稀疏格式\n",
    "        raw_counts = raw_counts.toarray()  # 转成密集数组\n",
    "    raw_counts = np.asarray(raw_counts, dtype=np.float32)  # 转为 float32 ndarray\n",
    "    if np.any(np.isnan(raw_counts)):  # 如果存在 NaN\n",
    "        raise ValueError(\"原始计数中存在 NaN，请检查输入文件\")  # 提示并中断\n",
    "    if (raw_counts < 0).any():  # 如果存在负值\n",
    "        raise ValueError(\"原始计数中存在负值，请检查输入文件\")  # 提示并中断\n",
    "\n",
    "    # 4) 计算 size_factors（library size），并归一化到几何平均为 1（常用做法）\n",
    "    total_counts = raw_counts.sum(axis=1)  # 每个细胞的总计数\n",
    "    total_counts[total_counts <= 0] = 1.0  # 避免零或负值导致除零\n",
    "    size_factors = total_counts / np.exp(np.mean(np.log(total_counts + 1e-8)))  # 使用几何平均归一化得到 size_factors\n",
    "    size_factors = size_factors.astype(np.float32)  # 转为 float32\n",
    "\n",
    "    # 5) 构建 DataLoader\n",
    "    dataset = CountsDataset(raw_counts, size_factors)  # 使用 CountsDataset 包装原始计数与 size_factors\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=False)  # 训练 DataLoader（打乱数据）\n",
    "\n",
    "    # 6) 初始化完整 ZINB-VAE 并设置优化器\n",
    "    n_genes = raw_counts.shape[1]  # 基因数\n",
    "    model = ZINBVAE(n_genes=n_genes, n_latent=latent_dim, hidden_dims=(256,128), theta_init=1.0, dropout=0.2)  # 初始化模型\n",
    "    model.to(device)  # 将模型移动到前置选择的设备\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)  # 使用 Adam 优化器\n",
    "\n",
    "    # 7) 训练模型（注意：绝不可使用 adata_hvg.obs['cell_type'] 参与训练）\n",
    "    model = train(model, dataloader, optimizer, device, n_epochs=n_epochs, kl_weight=1.0, verbose=True)  # 执行训练\n",
    "\n",
    "    # 8) 推断全部细胞的潜在表示 z_mu 并写回 adata_hvg.obsm\n",
    "    embeddings = get_latent_embeddings(model, raw_counts, size_factors, device, batch_size=256)  # 获取嵌入\n",
    "    adata_hvg.obsm['X_zinbvae'] = embeddings  # 将嵌入写回 prelude 的 adata（HVG 子集）\n",
    "\n",
    "    # 9) 聚类（可以使用真实标签的类别数来设置簇数，但真实标签不得用于训练）\n",
    "  # 确认用于评估的真实标签列：优先使用 'cell_type'，其次尝试 'clusters'，再尝试常见备选项\n",
    "    if 'cell_type' in adata_hvg.obs:\n",
    "        label_key = 'cell_type'  # 优先使用 cell_type\n",
    "    elif 'clusters' in adata_hvg.obs:\n",
    "        label_key = 'clusters'  # 回退使用 clusters（你提示中 labels 在 obs 的 clusters 列）\n",
    "    else:\n",
    "        # 尝试一些常见的备选列名\n",
    "        fallback_candidates = ['celltype', 'cell_type_pred', 'label', 'labels', 'true_labels']\n",
    "        label_key = None\n",
    "        for cand in fallback_candidates:\n",
    "            if cand in adata_hvg.obs:\n",
    "                label_key = cand\n",
    "                break\n",
    "        if label_key is None:\n",
    "            # 若没有找到任何候选列，则给出明确错误提示并列出可用的 obs 列\n",
    "            available = \", \".join(list(adata_hvg.obs.columns))\n",
    "            raise KeyError(\n",
    "                \"未在 adata.obs 中找到用于评估的真实标签列（例如 'cell_type' 或 'clusters'）。\"\n",
    "                f\" 请检查你的 AnnData，当前可用的 obs 列有: {available}\"\n",
    "            )\n",
    "    # 使用选定的标签列来确定簇数和构建 true_labels\n",
    "    true_labels = adata_hvg.obs[label_key].values.astype(str)  # 取出真实标签（字符串形式）\n",
    "    n_clusters = int(adata_hvg.obs[label_key].nunique())  # 使用真实标签的唯一值数作为聚类数\n",
    "    print(f\"使用 adata.obs['{label_key}'] 作为真实标签用于评估与确定簇数。\")  # 打印所用列名以便确认\n",
    "    print(f\"将使用 {n_clusters} 个簇对嵌入进行聚类（GaussianMixture）\")  # 打印聚类信息\n",
    "    gmm = GaussianMixture(n_components=n_clusters, covariance_type='full', random_state=112)  # 初始化 GMM\n",
    "    pred_labels = gmm.fit_predict(embeddings)  # 对嵌入执行聚类并获得预测簇标签\n",
    "    adata_hvg.obs['zinbvae_cluster'] = [str(int(x)) for x in pred_labels]  # 将预测簇标签存回 adata.obs（字符串形式）\n",
    "\n",
    "    # 10) 评估：计算 ARI 与 NMI（使用真实标签，但仅用于评估）\n",
    "    true_labels = adata_hvg.obs['clusters'].values.astype(str)  # 获取真实标签数组并转为字符串\n",
    "    ari = adjusted_rand_score(true_labels, pred_labels)  # 计算调整后的兰德指数（ARI）\n",
    "    nmi = normalized_mutual_info_score(true_labels, pred_labels, average_method='arithmetic')  # 计算归一化互信息（NMI）\n",
    "    print(\"聚类评估结果（基于 adata.obs['cell_type']）：\")  # 说明评估基准\n",
    "    print(f\"Adjusted Rand Index (ARI): {ari:.4f}\")  # 打印 ARI\n",
    "    print(f\"Normalized Mutual Information (NMI): {nmi:.4f}\")  # 打印 NMI\n",
    "    \n",
    "        # -------------------------\n",
    "    # 可视化（加入的位置：就在打印 ARI / NMI 之后，在保存 adata 之前）\n",
    "    # 说明：这里优先尝试使用 umap-learn；如果不可用则退回到 sklearn.manifold.TSNE\n",
    "    # -------------------------\n",
    "    try:\n",
    "        import umap as _umap  # 尝试导入 umap-learn\n",
    "        reducer = _umap.UMAP(n_components=2, random_state=42)  # 使用 UMAP 将高维嵌入降到 2 维\n",
    "        coords_2d = reducer.fit_transform(embeddings)  # 计算 2D 投影\n",
    "    except Exception:\n",
    "        from sklearn.manifold import TSNE  # 若 umap 不可用则使用 t-SNE 作为后备\n",
    "        reducer = TSNE(n_components=2, random_state=42, init='pca')  # 初始化 t-SNE\n",
    "        coords_2d = reducer.fit_transform(embeddings)  # 计算 2D 投影\n",
    "\n",
    "    # 存储 2D 投影到 adata.obsm，便于后续使用\n",
    "    adata_hvg.obsm['X_umap'] = coords_2d  # 将 2D 坐标保存为 X_umap（命名为 umap 方便与 scanpy 接口兼容）\n",
    "\n",
    "    # 绘制两个子图：左侧为预测簇标签，右侧为真实细胞类型\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))  # 创建画布和两个子图\n",
    "    # 颜色映射 - predicted clusters\n",
    "    uniq_pred = np.unique(pred_labels)  # 预测簇的唯一值\n",
    "    cmap_pred = cm.get_cmap('tab20', len(uniq_pred))  # 使用 tab20 colormap（或根据簇数量自动扩展）\n",
    "    pred_color_idxs = np.array([np.where(uniq_pred == v)[0][0] for v in pred_labels])  # 将簇标签映射到颜色索引\n",
    "    axes[0].scatter(coords_2d[:, 0], coords_2d[:, 1], c=cmap_pred(pred_color_idxs), s=6, linewidths=0)  # 绘制散点图\n",
    "    axes[0].set_title('ZINB-VAE clusters (predicted)')  # 子图标题\n",
    "    axes[0].axis('off')  # 关闭坐标轴以便更清晰展示\n",
    "\n",
    "    # 颜色映射 - true labels\n",
    "    uniq_true = np.unique(true_labels)  # 真实标签的唯一值\n",
    "    cmap_true = cm.get_cmap('tab20', len(uniq_true))  # 使用 tab20 colormap\n",
    "    true_color_idxs = np.array([np.where(uniq_true == v)[0][0] for v in true_labels])  # 将真实标签映射为颜色索引\n",
    "    axes[1].scatter(coords_2d[:, 0], coords_2d[:, 1], c=cmap_true(true_color_idxs), s=6, linewidths=0)  # 绘制散点图\n",
    "    axes[1].set_title('True cell types')  # 子图标题\n",
    "    axes[1].axis('off')  # 关闭坐标轴\n",
    "\n",
    "    plt.tight_layout()  # 自动调整子图间距\n",
    "    # 保存可视化图片到与输入 h5ad 相同目录下\n",
    "    vis_path = os.path.splitext(raw_h5ad_path)[0] + \"_zinbvae_clusters_umap.png\"  # 构造输出图片路径\n",
    "    plt.savefig(vis_path, dpi=150, bbox_inches='tight')  # 保存图片文件\n",
    "    plt.close(fig)  # 关闭 figure 以释放内存\n",
    "    print(f\"已保存聚类可视化图片到: {vis_path}\")  # 打印可视化文件路径\n",
    "\n",
    "    # 11) 可选：保存包含嵌入与聚类标签的 AnnData 文件\n",
    "    if out_h5ad is None:  # 如果用户未指定输出路径\n",
    "        out_h5ad = os.path.splitext(raw_h5ad_path)[0] + \"_zinbvae_full_out.h5ad\"  # 生成默认输出文件名\n",
    "    adata_hvg.write(out_h5ad)  # 将包含嵌入与簇标签的 AnnData 写入磁盘\n",
    "    print(f\"已将包含嵌入与簇标签的 AnnData 保存到: {out_h5ad}\")  # 打印保存路径\n",
    "    return adata_hvg, model, ari, nmi  # 返回结果以便进一步分析或测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a9c9c9c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500, avg_nll=1152.6655, avg_kl=0.9922\n",
      "Epoch 10/500, avg_nll=795.0070, avg_kl=4.0031\n",
      "Epoch 20/500, avg_nll=777.5748, avg_kl=4.3845\n",
      "Epoch 30/500, avg_nll=756.2423, avg_kl=5.9987\n",
      "Epoch 40/500, avg_nll=748.7731, avg_kl=5.9784\n",
      "Epoch 50/500, avg_nll=744.2017, avg_kl=6.1457\n",
      "Epoch 60/500, avg_nll=739.9522, avg_kl=6.5028\n",
      "Epoch 70/500, avg_nll=735.4031, avg_kl=6.5612\n",
      "Epoch 80/500, avg_nll=733.8824, avg_kl=6.5912\n",
      "Epoch 90/500, avg_nll=730.8128, avg_kl=6.5701\n",
      "Epoch 100/500, avg_nll=731.4570, avg_kl=6.6804\n",
      "Epoch 110/500, avg_nll=730.2116, avg_kl=6.6004\n",
      "Epoch 120/500, avg_nll=728.1441, avg_kl=6.6634\n",
      "Epoch 130/500, avg_nll=726.6465, avg_kl=6.6367\n",
      "Epoch 140/500, avg_nll=726.9549, avg_kl=6.6911\n",
      "Epoch 150/500, avg_nll=725.9713, avg_kl=6.6061\n",
      "Epoch 160/500, avg_nll=725.5042, avg_kl=6.6588\n",
      "Epoch 170/500, avg_nll=724.5407, avg_kl=6.6075\n",
      "Epoch 180/500, avg_nll=723.3138, avg_kl=6.6369\n",
      "Epoch 190/500, avg_nll=722.8658, avg_kl=6.6247\n",
      "Epoch 200/500, avg_nll=721.8961, avg_kl=6.7770\n",
      "Epoch 210/500, avg_nll=721.6985, avg_kl=6.7763\n",
      "Epoch 220/500, avg_nll=721.0625, avg_kl=6.8012\n",
      "Epoch 230/500, avg_nll=721.4057, avg_kl=6.7521\n",
      "Epoch 240/500, avg_nll=719.1209, avg_kl=6.9180\n",
      "Epoch 250/500, avg_nll=719.2222, avg_kl=6.7654\n",
      "Epoch 260/500, avg_nll=719.2973, avg_kl=6.9049\n",
      "Epoch 270/500, avg_nll=718.5690, avg_kl=6.9069\n",
      "Epoch 280/500, avg_nll=717.8398, avg_kl=6.9511\n",
      "Epoch 290/500, avg_nll=718.3987, avg_kl=6.9679\n",
      "Epoch 300/500, avg_nll=717.3293, avg_kl=6.9682\n",
      "Epoch 310/500, avg_nll=717.8824, avg_kl=6.9219\n",
      "Epoch 320/500, avg_nll=717.1322, avg_kl=6.9954\n",
      "Epoch 330/500, avg_nll=715.7660, avg_kl=7.0022\n",
      "Epoch 340/500, avg_nll=714.9481, avg_kl=7.0844\n",
      "Epoch 350/500, avg_nll=715.3631, avg_kl=7.1188\n",
      "Epoch 360/500, avg_nll=716.9253, avg_kl=7.0754\n",
      "Epoch 370/500, avg_nll=714.9313, avg_kl=7.0594\n",
      "Epoch 380/500, avg_nll=714.7384, avg_kl=7.1846\n",
      "Epoch 390/500, avg_nll=713.5237, avg_kl=7.1792\n",
      "Epoch 400/500, avg_nll=713.7303, avg_kl=7.1168\n",
      "Epoch 410/500, avg_nll=713.7980, avg_kl=7.1770\n",
      "Epoch 420/500, avg_nll=714.6902, avg_kl=7.2159\n",
      "Epoch 430/500, avg_nll=712.8849, avg_kl=7.2934\n",
      "Epoch 440/500, avg_nll=715.3576, avg_kl=7.2873\n",
      "Epoch 450/500, avg_nll=714.0679, avg_kl=7.2595\n",
      "Epoch 460/500, avg_nll=712.1600, avg_kl=7.2965\n",
      "Epoch 470/500, avg_nll=712.5919, avg_kl=7.2951\n",
      "Epoch 480/500, avg_nll=712.5007, avg_kl=7.3566\n",
      "Epoch 490/500, avg_nll=710.2856, avg_kl=7.2953\n",
      "Epoch 500/500, avg_nll=711.4753, avg_kl=7.4062\n",
      "使用 adata.obs['clusters'] 作为真实标签用于评估与确定簇数。\n",
      "将使用 10 个簇对嵌入进行聚类（GaussianMixture）\n",
      "聚类评估结果（基于 adata.obs['cell_type']）：\n",
      "Adjusted Rand Index (ARI): 0.5110\n",
      "Normalized Mutual Information (NMI): 0.6122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniforge/envs/omic_env/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n",
      "/tmp/ipykernel_235479/3261810217.py:111: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
      "  cmap_pred = cm.get_cmap('tab20', len(uniq_pred))  # 使用 tab20 colormap（或根据簇数量自动扩展）\n",
      "/tmp/ipykernel_235479/3261810217.py:119: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
      "  cmap_true = cm.get_cmap('tab20', len(uniq_true))  # 使用 tab20 colormap\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已保存聚类可视化图片到: /share/2025_undergraduate/20251106/setty_bone_marrow_zinbvae_clusters_umap.png\n",
      "已将包含嵌入与簇标签的 AnnData 保存到: /share/2025_undergraduate/20251106/setty_bone_marrow_zinbvae_full_out.h5ad\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":  # 如果脚本作为主程序运行则执行下面流程\n",
    "    # 直接使用前置定义的 adata 与 file_path（前置代码已将它们加载）\n",
    "    adata_result, trained_model, ari_value, nmi_value = run_full_from_prelude(\n",
    "        adata_prelude=adata,  # 前置步骤处理后的 AnnData（HVG 子集）\n",
    "        raw_h5ad_path=file_path,  # 原始 h5ad 文件路径以提取原始计数\n",
    "        device=device,  # 使用你前置选择的设备\n",
    "        out_h5ad=None,  # 使用默认输出文件名（可修改为你想要的路径）\n",
    "        latent_dim=10,  # 潜变量维度（可根据需要调整）\n",
    "        batch_size=128,  # 训练批次大小（可调整）\n",
    "        n_epochs=500,  # 训练轮数（可调整）\n",
    "        lr=1e-4,  # 学习率（可调整）\n",
    "        seed = 42  # 随机种子以保证可重复性\n",
    "    )  # 执行完整流程并返回结果（包括已保存的 adata）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdcaf81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.5806e-01,  1.4061e-02,  4.6783e-02, ..., -2.5238e-02,\n",
       "         8.0273e-01, -1.2256e-01],\n",
       "       [ 6.6101e-02, -1.6998e-02,  2.4014e-03, ..., -9.7227e-04,\n",
       "        -4.3872e-01,  1.7419e-01],\n",
       "       [-3.0908e-01, -1.1774e-01, -3.9795e-02, ..., -3.7292e-02,\n",
       "        -9.7607e-01,  8.2947e-02],\n",
       "       ...,\n",
       "       [ 2.1936e-01,  1.9177e-01,  1.6266e-02, ..., -3.9077e-04,\n",
       "         2.1399e-01, -4.4983e-02],\n",
       "       [ 5.7861e-01,  2.1362e-01,  6.8726e-02, ..., -3.8300e-02,\n",
       "         1.0107e+00, -3.6255e-01],\n",
       "       [ 2.1790e-01,  1.6309e-01,  8.5983e-03, ...,  4.0863e-02,\n",
       "         2.4246e-02,  1.6998e-02]], shape=(5780, 14319), dtype=float16)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata.obsm['MAGIC_imputed_data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3818b317",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.36255"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-3.6255e-01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8b958486",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.036255"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-3.6255e-02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e0fc4e73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.36255"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-3.6255e-01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcdf649b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "omic_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
